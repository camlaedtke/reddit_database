{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d43b8bc6",
   "metadata": {},
   "source": [
    "# Reddit Comment Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1734bc00",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import json\n",
    "import sqlite3\n",
    "import sqlalchemy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from time import time\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3f94ea08",
   "metadata": {},
   "outputs": [],
   "source": [
    "dtype_dict = {\n",
    "    \"author\": str,\n",
    "    \"author_fullname\": str,\n",
    "    \"awarders\": str,\n",
    "    \"body\": str,\n",
    "    \"id\": str,\n",
    "    \"link_id\": str,\n",
    "    \"subreddit\": str,\n",
    "    \"subreddit_id\": str,\n",
    "    \"subreddit_type\": str,\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "def count_objects_in_file(filepath):\n",
    "    \"\"\"Count how many comments are in a .json file\"\"\"\n",
    "    idx = 0\n",
    "    with open(filepath) as fp:\n",
    "        for line in fp:\n",
    "            idx+=1\n",
    "    return idx\n",
    "\n",
    "\n",
    "def extract_subset(filepath, start=0, end=10):\n",
    "    \"\"\"Extract a subset of raw comment data directly from .json file\"\"\"\n",
    "    comments = []\n",
    "    with open(filepath) as fp:\n",
    "        for idx, line in enumerate(fp):\n",
    "            if (idx >= start) and (idx < end):\n",
    "                comment = json.loads(line)\n",
    "                comments.append(comment)\n",
    "            elif idx >= end:\n",
    "                break                \n",
    "        return comments\n",
    "        \n",
    "    \n",
    "def write_to_database(db_conn, json_fp, chunk_size):\n",
    "    \"\"\"Write the contents of temporary .json file to SQLite database\"\"\"\n",
    "    batch_no=1\n",
    "    for chunk in pd.read_json(json_fp, chunksize=chunk_size, lines=True):\n",
    "        try: \n",
    "            chunk.to_sql('reddit_comments', db_conn, if_exists='append')\n",
    "        except sqlalchemy.exc.SQLAlchemyError as e: \n",
    "            print(\"\\n  {}\".format(e.orig))\n",
    "        batch_no+=1\n",
    "        \n",
    "        \n",
    "def drop_additional_columns(df):\n",
    "    \"\"\"Drops specific columns from the dataframe if they exist. This is necessay because some of the \n",
    "       comment archives contain additional columns.\"\"\"\n",
    "    if \"author_cakeday\" in df.columns:\n",
    "        df.drop(columns=\"author_cakeday\", inplace=True)\n",
    "    if \"comment_type\" in df.columns:\n",
    "        df.drop(columns=\"comment_type\", inplace=True)\n",
    "    if \"media_metadata\" in df.columns:\n",
    "        df.drop(columns=\"media_metadata\", inplace=True)\n",
    "    if \"editable\" in df.columns:\n",
    "        df.drop(columns=\"editable\", inplace=True)\n",
    "    return df\n",
    "    \n",
    "        \n",
    "        \n",
    "def create_database(database, json_fp, comments_per_chunk, chunk_size, columns_to_drop):\n",
    "    \"\"\"\n",
    "    args:\n",
    "        database            - sqlite databse object\n",
    "        json_fp             - filepath to .json raw comments file\n",
    "        comments_per_chunk  - number of comments to store in temporary .json files \n",
    "        chunk_size          - size of chunks for the pd.read_json() function\n",
    "        columns_to_drop     - columns to drop \n",
    "    \"\"\"\n",
    "    print(\"\\n######## File: {}\".format(json_fp))\n",
    "    n_comments_total = count_objects_in_file(filepath=json_fp)\n",
    "    n_chunks = (n_comments_total // comments_per_chunk) + 1\n",
    "    print(\"Contains {} comments - dividing into {} chunks\".format(n_comments_total, n_chunks))\n",
    "    \n",
    "    for idx in range(0, n_chunks):\n",
    "        start = int(idx * comments_per_chunk)\n",
    "        end = int(start + comments_per_chunk)\n",
    "        print(\"\\r   File chunk {}: Extracting and writing comments {} - {}\".format(idx+1, start, end), end='')\n",
    "        comments = extract_subset(filepath=json_fp, start=start, end=end)\n",
    "        df = pd.DataFrame(comments)\n",
    "        df = df.drop(columns=columns_to_drop)\n",
    "        df = drop_additional_columns(df=df)\n",
    "        df = df.astype(dtype_dict)\n",
    "        # Load the chunk into temporary .json file\n",
    "        df.to_json(\"data/db_chunk.json\", orient='records', lines=True)\n",
    "        write_to_database(\n",
    "            db_conn=database, \n",
    "            json_fp=\"data/db_chunk.json\", \n",
    "            chunk_size=chunk_size\n",
    "        )\n",
    "        \n",
    "        \n",
    "def run_database_builder(input_path, db_path, drop_cols, comments_per_chunk=500000, chunk_size=100000):\n",
    "    \"\"\"\n",
    "    args:\n",
    "        input_path          - filepath to .json raw comments file\n",
    "        db_path             - filepath to database file\n",
    "        drop_cols           - columns to drop \n",
    "        comments_per_chunk  - number of comments to store in temporary .json files \n",
    "        chunk_size          - size of chunks for the pd.read_json() function\n",
    "    \"\"\"\n",
    "    \n",
    "    conn = create_engine('sqlite:///'+db_path)\n",
    "\n",
    "    create_database(\n",
    "        database=conn, \n",
    "        json_fp=input_path,\n",
    "        comments_per_chunk=comments_per_chunk, \n",
    "        chunk_size=chunk_size,\n",
    "        columns_to_drop=drop_cols\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f1da12d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_ROOT = \"/media/cameron/Seagate Basic/datasets/reddit/archives_decompressed/*/*.json\"\n",
    "DATA_DEST = \"data/RC_2020_database.db\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "614d4428",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/media/cameron/Seagate Basic/datasets/reddit/archives_decompressed/jun_2020/RC_2020-06-13.json\n"
     ]
    }
   ],
   "source": [
    "data_files = sorted(glob.glob(DATA_ROOT))\n",
    "print(data_files[102])\n",
    "# n_comments = count_objects_in_file(filepath=data_files[0])\n",
    "# print(n_comments)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5f67879",
   "metadata": {},
   "source": [
    "Specify some irrelevant columns to drop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1ffe17b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_cols = [\"all_awardings\", \n",
    "             \"associated_award\", \n",
    "             \"author_flair_css_class\", \n",
    "             \"author_flair_richtext\",\n",
    "             \"author_flair_background_color\",\n",
    "             \"author_flair_text_color\",\n",
    "             \"author_flair_type\",\n",
    "             \"author_patreon_flair\",\n",
    "             \"author_flair_template_id\",\n",
    "             \"author_premium\",\n",
    "             \"can_gild\", \n",
    "             \"collapsed\",\n",
    "             \"collapsed_because_crowd_control\",\n",
    "             \"collapsed_reason\",\n",
    "             \"gildings\",\n",
    "             \"permalink\", \n",
    "             \"subreddit_name_prefixed\",\n",
    "             \"treatment_tags\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b3b5abf",
   "metadata": {},
   "source": [
    "Sanity check: Extract some comments from a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d89bfaa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['author', 'author_created_utc', 'author_flair_text', 'author_fullname', 'awarders', 'body', 'can_mod_post', 'controversiality', 'created_utc', 'distinguished', 'edited', 'gilded', 'id', 'is_submitter', 'link_id', 'locked', 'no_follow', 'parent_id', 'quarantined', 'removal_reason', 'retrieved_on', 'score', 'send_replies', 'stickied', 'subreddit', 'subreddit_id', 'subreddit_type', 'top_awarded_type', 'total_awards_received']\n"
     ]
    }
   ],
   "source": [
    "comments = extract_subset(filepath=data_files[104], start=0, end=100)\n",
    "df = pd.DataFrame(comments)\n",
    "df = df.drop(columns=drop_cols)\n",
    "df = drop_additional_columns(df=df)\n",
    "df = df.astype(dtype_dict)\n",
    "print(df.columns.values.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "640ed5a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>author_created_utc</th>\n",
       "      <th>author_flair_text</th>\n",
       "      <th>author_fullname</th>\n",
       "      <th>awarders</th>\n",
       "      <th>body</th>\n",
       "      <th>can_mod_post</th>\n",
       "      <th>controversiality</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>distinguished</th>\n",
       "      <th>edited</th>\n",
       "      <th>gilded</th>\n",
       "      <th>id</th>\n",
       "      <th>is_submitter</th>\n",
       "      <th>link_id</th>\n",
       "      <th>locked</th>\n",
       "      <th>no_follow</th>\n",
       "      <th>parent_id</th>\n",
       "      <th>quarantined</th>\n",
       "      <th>removal_reason</th>\n",
       "      <th>retrieved_on</th>\n",
       "      <th>score</th>\n",
       "      <th>send_replies</th>\n",
       "      <th>stickied</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>subreddit_id</th>\n",
       "      <th>subreddit_type</th>\n",
       "      <th>top_awarded_type</th>\n",
       "      <th>total_awards_received</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>KlutzyDesign</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>t2_6esnza6x</td>\n",
       "      <td>[]</td>\n",
       "      <td>I’m summarizing a bible story for you, what do...</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>1592179200</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>fuumj80</td>\n",
       "      <td>False</td>\n",
       "      <td>t3_h8ouxr</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>t1_fuumg1t</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>1600998230</td>\n",
       "      <td>3</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>insanepeoplefacebook</td>\n",
       "      <td>t5_3acf2</td>\n",
       "      <td>public</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>santropez1972</td>\n",
       "      <td>1.421422e+09</td>\n",
       "      <td>None</td>\n",
       "      <td>t2_kr2tq</td>\n",
       "      <td>[]</td>\n",
       "      <td>I know where I want it to go!</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>1592179200</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>fuumj81</td>\n",
       "      <td>False</td>\n",
       "      <td>t3_h93qni</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>t1_fuuirqk</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>1600998230</td>\n",
       "      <td>4</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>gilf</td>\n",
       "      <td>t5_2qvqa</td>\n",
       "      <td>public</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>kernals12</td>\n",
       "      <td>1.536085e+09</td>\n",
       "      <td>None</td>\n",
       "      <td>t2_23su9ek6</td>\n",
       "      <td>[]</td>\n",
       "      <td>Yet another sign that we should've taken Canada.</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>1592179200</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>fuumj82</td>\n",
       "      <td>False</td>\n",
       "      <td>t3_h94skb</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>t3_h94skb</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>1600998230</td>\n",
       "      <td>-1</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>MapPorn</td>\n",
       "      <td>t5_2si92</td>\n",
       "      <td>public</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>nicernicer</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>t2_6982ih24</td>\n",
       "      <td>[]</td>\n",
       "      <td>nice</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>1592179200</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>fuumj83</td>\n",
       "      <td>False</td>\n",
       "      <td>t3_h94ssz</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>t3_h94ssz</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>1600998230</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>lookatmydog</td>\n",
       "      <td>t5_2s6t5</td>\n",
       "      <td>public</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>amcg10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Bootlegger :Bootlegger:</td>\n",
       "      <td>t2_45mylopt</td>\n",
       "      <td>[]</td>\n",
       "      <td>Okay fair enough you raise a good point still ...</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>1592179200</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>fuumj84</td>\n",
       "      <td>False</td>\n",
       "      <td>t3_h8wgtx</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>t1_fuum7h3</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>1600998230</td>\n",
       "      <td>4</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>apexlegends</td>\n",
       "      <td>t5_rgzzt</td>\n",
       "      <td>public</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          author  author_created_utc        author_flair_text author_fullname  \\\n",
       "0   KlutzyDesign                 NaN                     None     t2_6esnza6x   \n",
       "1  santropez1972        1.421422e+09                     None        t2_kr2tq   \n",
       "2      kernals12        1.536085e+09                     None     t2_23su9ek6   \n",
       "3     nicernicer                 NaN                     None     t2_6982ih24   \n",
       "4         amcg10                 NaN  Bootlegger :Bootlegger:     t2_45mylopt   \n",
       "\n",
       "  awarders                                               body  can_mod_post  \\\n",
       "0       []  I’m summarizing a bible story for you, what do...         False   \n",
       "1       []                      I know where I want it to go!         False   \n",
       "2       []   Yet another sign that we should've taken Canada.         False   \n",
       "3       []                                               nice         False   \n",
       "4       []  Okay fair enough you raise a good point still ...         False   \n",
       "\n",
       "   controversiality  created_utc distinguished edited  gilded       id  \\\n",
       "0                 0   1592179200          None  False       0  fuumj80   \n",
       "1                 0   1592179200          None  False       0  fuumj81   \n",
       "2                 0   1592179200          None  False       0  fuumj82   \n",
       "3                 0   1592179200          None  False       0  fuumj83   \n",
       "4                 0   1592179200          None  False       0  fuumj84   \n",
       "\n",
       "   is_submitter    link_id  locked  no_follow   parent_id  quarantined  \\\n",
       "0         False  t3_h8ouxr   False       True  t1_fuumg1t        False   \n",
       "1         False  t3_h93qni   False      False  t1_fuuirqk        False   \n",
       "2         False  t3_h94skb   False       True   t3_h94skb        False   \n",
       "3         False  t3_h94ssz   False       True   t3_h94ssz        False   \n",
       "4         False  t3_h8wgtx   False      False  t1_fuum7h3        False   \n",
       "\n",
       "  removal_reason  retrieved_on  score  send_replies  stickied  \\\n",
       "0           None    1600998230      3          True     False   \n",
       "1           None    1600998230      4          True     False   \n",
       "2           None    1600998230     -1          True     False   \n",
       "3           None    1600998230      2          True     False   \n",
       "4           None    1600998230      4          True     False   \n",
       "\n",
       "              subreddit subreddit_id subreddit_type top_awarded_type  \\\n",
       "0  insanepeoplefacebook     t5_3acf2         public             None   \n",
       "1                  gilf     t5_2qvqa         public             None   \n",
       "2               MapPorn     t5_2si92         public             None   \n",
       "3           lookatmydog     t5_2s6t5         public             None   \n",
       "4           apexlegends     t5_rgzzt         public             None   \n",
       "\n",
       "   total_awards_received  \n",
       "0                      0  \n",
       "1                      0  \n",
       "2                      0  \n",
       "3                      0  \n",
       "4                      0  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1fee2eda",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/media/cameron/Seagate Basic/datasets/reddit/archives_decompressed/jun_2020/RC_2020-06-13.json',\n",
       " '/media/cameron/Seagate Basic/datasets/reddit/archives_decompressed/jun_2020/RC_2020-06-14.json',\n",
       " '/media/cameron/Seagate Basic/datasets/reddit/archives_decompressed/jun_2020/RC_2020-06-15.json',\n",
       " '/media/cameron/Seagate Basic/datasets/reddit/archives_decompressed/jun_2020/RC_2020-06-16.json',\n",
       " '/media/cameron/Seagate Basic/datasets/reddit/archives_decompressed/jun_2020/RC_2020-06-17.json',\n",
       " '/media/cameron/Seagate Basic/datasets/reddit/archives_decompressed/jun_2020/RC_2020-06-18.json',\n",
       " '/media/cameron/Seagate Basic/datasets/reddit/archives_decompressed/jun_2020/RC_2020-06-19.json',\n",
       " '/media/cameron/Seagate Basic/datasets/reddit/archives_decompressed/jun_2020/RC_2020-06-20.json',\n",
       " '/media/cameron/Seagate Basic/datasets/reddit/archives_decompressed/jun_2020/RC_2020-06-21.json',\n",
       " '/media/cameron/Seagate Basic/datasets/reddit/archives_decompressed/jun_2020/RC_2020-06-22.json',\n",
       " '/media/cameron/Seagate Basic/datasets/reddit/archives_decompressed/jun_2020/RC_2020-06-23.json',\n",
       " '/media/cameron/Seagate Basic/datasets/reddit/archives_decompressed/jun_2020/RC_2020-06-24.json',\n",
       " '/media/cameron/Seagate Basic/datasets/reddit/archives_decompressed/jun_2020/RC_2020-06-25.json',\n",
       " '/media/cameron/Seagate Basic/datasets/reddit/archives_decompressed/jun_2020/RC_2020-06-26.json',\n",
       " '/media/cameron/Seagate Basic/datasets/reddit/archives_decompressed/jun_2020/RC_2020-06-27.json',\n",
       " '/media/cameron/Seagate Basic/datasets/reddit/archives_decompressed/jun_2020/RC_2020-06-28.json',\n",
       " '/media/cameron/Seagate Basic/datasets/reddit/archives_decompressed/jun_2020/RC_2020-06-29.json',\n",
       " '/media/cameron/Seagate Basic/datasets/reddit/archives_decompressed/jun_2020/RC_2020-06-30.json',\n",
       " '/media/cameron/Seagate Basic/datasets/reddit/archives_decompressed/mar_2020/RC_2020-03-01.json',\n",
       " '/media/cameron/Seagate Basic/datasets/reddit/archives_decompressed/mar_2020/RC_2020-03-02.json',\n",
       " '/media/cameron/Seagate Basic/datasets/reddit/archives_decompressed/mar_2020/RC_2020-03-04.json',\n",
       " '/media/cameron/Seagate Basic/datasets/reddit/archives_decompressed/mar_2020/RC_2020-03-05.json',\n",
       " '/media/cameron/Seagate Basic/datasets/reddit/archives_decompressed/mar_2020/RC_2020-03-06.json',\n",
       " '/media/cameron/Seagate Basic/datasets/reddit/archives_decompressed/mar_2020/RC_2020-03-07.json',\n",
       " '/media/cameron/Seagate Basic/datasets/reddit/archives_decompressed/mar_2020/RC_2020-03-08.json',\n",
       " '/media/cameron/Seagate Basic/datasets/reddit/archives_decompressed/mar_2020/RC_2020-03-09.json',\n",
       " '/media/cameron/Seagate Basic/datasets/reddit/archives_decompressed/mar_2020/RC_2020-03-10.json',\n",
       " '/media/cameron/Seagate Basic/datasets/reddit/archives_decompressed/mar_2020/RC_2020-03-11.json',\n",
       " '/media/cameron/Seagate Basic/datasets/reddit/archives_decompressed/mar_2020/RC_2020-03-12.json',\n",
       " '/media/cameron/Seagate Basic/datasets/reddit/archives_decompressed/mar_2020/RC_2020-03-13.json',\n",
       " '/media/cameron/Seagate Basic/datasets/reddit/archives_decompressed/mar_2020/RC_2020-03-14.json',\n",
       " '/media/cameron/Seagate Basic/datasets/reddit/archives_decompressed/mar_2020/RC_2020-03-15.json',\n",
       " '/media/cameron/Seagate Basic/datasets/reddit/archives_decompressed/mar_2020/RC_2020-03-16.json',\n",
       " '/media/cameron/Seagate Basic/datasets/reddit/archives_decompressed/mar_2020/RC_2020-03-17.json',\n",
       " '/media/cameron/Seagate Basic/datasets/reddit/archives_decompressed/mar_2020/RC_2020-03-18.json',\n",
       " '/media/cameron/Seagate Basic/datasets/reddit/archives_decompressed/mar_2020/RC_2020-03-19.json',\n",
       " '/media/cameron/Seagate Basic/datasets/reddit/archives_decompressed/mar_2020/RC_2020-03-20.json',\n",
       " '/media/cameron/Seagate Basic/datasets/reddit/archives_decompressed/mar_2020/RC_2020-03-21.json',\n",
       " '/media/cameron/Seagate Basic/datasets/reddit/archives_decompressed/mar_2020/RC_2020-03-22.json',\n",
       " '/media/cameron/Seagate Basic/datasets/reddit/archives_decompressed/mar_2020/RC_2020-03-23.json',\n",
       " '/media/cameron/Seagate Basic/datasets/reddit/archives_decompressed/mar_2020/RC_2020-03-24.json',\n",
       " '/media/cameron/Seagate Basic/datasets/reddit/archives_decompressed/mar_2020/RC_2020-03-25.json',\n",
       " '/media/cameron/Seagate Basic/datasets/reddit/archives_decompressed/mar_2020/RC_2020-03-26.json',\n",
       " '/media/cameron/Seagate Basic/datasets/reddit/archives_decompressed/mar_2020/RC_2020-03-27.json',\n",
       " '/media/cameron/Seagate Basic/datasets/reddit/archives_decompressed/mar_2020/RC_2020-03-28.json',\n",
       " '/media/cameron/Seagate Basic/datasets/reddit/archives_decompressed/mar_2020/RC_2020-03-29.json',\n",
       " '/media/cameron/Seagate Basic/datasets/reddit/archives_decompressed/mar_2020/RC_2020-03-30.json',\n",
       " '/media/cameron/Seagate Basic/datasets/reddit/archives_decompressed/mar_2020/RC_2020-03-31.json',\n",
       " '/media/cameron/Seagate Basic/datasets/reddit/archives_decompressed/may_2020/RC_2020-05-01.json',\n",
       " '/media/cameron/Seagate Basic/datasets/reddit/archives_decompressed/may_2020/RC_2020-05-02.json',\n",
       " '/media/cameron/Seagate Basic/datasets/reddit/archives_decompressed/may_2020/RC_2020-05-03.json',\n",
       " '/media/cameron/Seagate Basic/datasets/reddit/archives_decompressed/may_2020/RC_2020-05-04.json',\n",
       " '/media/cameron/Seagate Basic/datasets/reddit/archives_decompressed/may_2020/RC_2020-05-05.json',\n",
       " '/media/cameron/Seagate Basic/datasets/reddit/archives_decompressed/may_2020/RC_2020-05-06.json',\n",
       " '/media/cameron/Seagate Basic/datasets/reddit/archives_decompressed/may_2020/RC_2020-05-07.json',\n",
       " '/media/cameron/Seagate Basic/datasets/reddit/archives_decompressed/may_2020/RC_2020-05-08.json',\n",
       " '/media/cameron/Seagate Basic/datasets/reddit/archives_decompressed/may_2020/RC_2020-05-09.json',\n",
       " '/media/cameron/Seagate Basic/datasets/reddit/archives_decompressed/may_2020/RC_2020-05-10.json',\n",
       " '/media/cameron/Seagate Basic/datasets/reddit/archives_decompressed/may_2020/RC_2020-05-11.json',\n",
       " '/media/cameron/Seagate Basic/datasets/reddit/archives_decompressed/may_2020/RC_2020-05-12.json',\n",
       " '/media/cameron/Seagate Basic/datasets/reddit/archives_decompressed/may_2020/RC_2020-05-13.json',\n",
       " '/media/cameron/Seagate Basic/datasets/reddit/archives_decompressed/may_2020/RC_2020-05-14.json',\n",
       " '/media/cameron/Seagate Basic/datasets/reddit/archives_decompressed/may_2020/RC_2020-05-15.json',\n",
       " '/media/cameron/Seagate Basic/datasets/reddit/archives_decompressed/may_2020/RC_2020-05-16.json',\n",
       " '/media/cameron/Seagate Basic/datasets/reddit/archives_decompressed/may_2020/RC_2020-05-17.json',\n",
       " '/media/cameron/Seagate Basic/datasets/reddit/archives_decompressed/may_2020/RC_2020-05-18.json',\n",
       " '/media/cameron/Seagate Basic/datasets/reddit/archives_decompressed/may_2020/RC_2020-05-19.json',\n",
       " '/media/cameron/Seagate Basic/datasets/reddit/archives_decompressed/may_2020/RC_2020-05-20.json',\n",
       " '/media/cameron/Seagate Basic/datasets/reddit/archives_decompressed/may_2020/RC_2020-05-21.json',\n",
       " '/media/cameron/Seagate Basic/datasets/reddit/archives_decompressed/may_2020/RC_2020-05-22.json',\n",
       " '/media/cameron/Seagate Basic/datasets/reddit/archives_decompressed/may_2020/RC_2020-05-23.json',\n",
       " '/media/cameron/Seagate Basic/datasets/reddit/archives_decompressed/may_2020/RC_2020-05-24.json',\n",
       " '/media/cameron/Seagate Basic/datasets/reddit/archives_decompressed/may_2020/RC_2020-05-25.json',\n",
       " '/media/cameron/Seagate Basic/datasets/reddit/archives_decompressed/may_2020/RC_2020-05-26.json',\n",
       " '/media/cameron/Seagate Basic/datasets/reddit/archives_decompressed/may_2020/RC_2020-05-27.json',\n",
       " '/media/cameron/Seagate Basic/datasets/reddit/archives_decompressed/may_2020/RC_2020-05-28.json',\n",
       " '/media/cameron/Seagate Basic/datasets/reddit/archives_decompressed/may_2020/RC_2020-05-29.json',\n",
       " '/media/cameron/Seagate Basic/datasets/reddit/archives_decompressed/may_2020/RC_2020-05-30.json',\n",
       " '/media/cameron/Seagate Basic/datasets/reddit/archives_decompressed/may_2020/RC_2020-05-31.json']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_files[102:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7adb8d1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "######## File: /media/cameron/Seagate Basic/datasets/reddit/archives_decompressed/jun_2020/RC_2020-06-13.json\n",
      "Contains 5834459 comments - dividing into 12 chunks\n",
      "   File chunk 12: Extracting and writing comments 5500000 - 6000000Finished. Time: 28.0 min\n",
      "\n",
      "######## File: /media/cameron/Seagate Basic/datasets/reddit/archives_decompressed/jun_2020/RC_2020-06-14.json\n",
      "Contains 5790092 comments - dividing into 12 chunks\n",
      "   File chunk 12: Extracting and writing comments 5500000 - 6000000Finished. Time: 46.6 min\n",
      "\n",
      "######## File: /media/cameron/Seagate Basic/datasets/reddit/archives_decompressed/jun_2020/RC_2020-06-15.json\n",
      "Contains 6423523 comments - dividing into 13 chunks\n",
      "   File chunk 3: Extracting and writing comments 1000000 - 1500000"
     ]
    }
   ],
   "source": [
    "for idx, data_file in enumerate(data_files[102:]):\n",
    "    t_start = time()\n",
    "    run_database_builder(\n",
    "        input_path=data_file, \n",
    "        db_path=DATA_DEST, \n",
    "        drop_cols=drop_cols,\n",
    "    )\n",
    "    t_end = time()\n",
    "    t_iter = (t_end-t_start)/60\n",
    "    print(\"\\nFinished. Time: {:.1f} min\".format(t_iter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9f26f8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# t_start = time()\n",
    "# run_database_builder(\n",
    "#     input_path=data_files[101], \n",
    "#     db_path=DATA_DEST, \n",
    "#     drop_cols=drop_cols,\n",
    "# )\n",
    "# t_end = time()\n",
    "# t_iter = (t_end-t_start)/60\n",
    "# print(\"Finished. Time: {:.1f} min\".format(t_iter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b970d169",
   "metadata": {},
   "outputs": [],
   "source": [
    "DB_NAME = DATA_DEST\n",
    "conn = sqlite3.connect(DB_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efc3be36",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_sql_query(\"\"\"SELECT * FROM reddit_comments LIMIT 100\"\"\", conn)\n",
    "df = df.drop(columns=[\"index\"])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a915285",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
