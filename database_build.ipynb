{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d43b8bc6",
   "metadata": {},
   "source": [
    "# Reddit Comment Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1734bc00",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import json\n",
    "import sqlite3\n",
    "import sqlalchemy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from time import time\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3f94ea08",
   "metadata": {},
   "outputs": [],
   "source": [
    "dtype_dict = {\n",
    "    \"author\": str,\n",
    "    \"author_fullname\": str,\n",
    "    \"awarders\": str,\n",
    "    \"body\": str,\n",
    "    \"id\": str,\n",
    "    \"link_id\": str,\n",
    "    \"subreddit\": str,\n",
    "    \"subreddit_id\": str,\n",
    "    \"subreddit_type\": str,\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "def count_objects_in_file(filepath):\n",
    "    \"\"\"Count how many comments are in a .json file\"\"\"\n",
    "    idx = 0\n",
    "    with open(filepath) as fp:\n",
    "        for line in fp:\n",
    "            idx+=1\n",
    "    return idx\n",
    "\n",
    "\n",
    "def extract_subset(filepath, start=0, end=10):\n",
    "    \"\"\"Extract a subset of raw comment data directly from .json file\"\"\"\n",
    "    comments = []\n",
    "    with open(filepath) as fp:\n",
    "        for idx, line in enumerate(fp):\n",
    "            if (idx >= start) and (idx < end):\n",
    "                comment = json.loads(line)\n",
    "                comments.append(comment)\n",
    "            elif idx >= end:\n",
    "                break                \n",
    "        return comments\n",
    "        \n",
    "    \n",
    "def write_to_database(db_conn, json_fp, chunk_size):\n",
    "    \"\"\"Write the contents of temporary .json file to SQLite database\"\"\"\n",
    "    batch_no=1\n",
    "    for chunk in pd.read_json(json_fp, chunksize=chunk_size, lines=True):\n",
    "        try: \n",
    "            chunk.to_sql('reddit_comments', db_conn, if_exists='append')\n",
    "        except sqlalchemy.exc.SQLAlchemyError as e: \n",
    "            print(\"\\n  {}\".format(e.orig))\n",
    "        batch_no+=1\n",
    "        \n",
    "        \n",
    "def drop_additional_columns(df):\n",
    "    \"\"\"Drops specific columns from the dataframe if they exist. This is necessay because some of the \n",
    "       comment archives contain additional columns.\"\"\"\n",
    "    if \"author_cakeday\" in df.columns:\n",
    "        df.drop(columns=\"author_cakeday\", inplace=True)\n",
    "    if \"comment_type\" in df.columns:\n",
    "        df.drop(columns=\"comment_type\", inplace=True)\n",
    "    if \"media_metadata\" in df.columns:\n",
    "        df.drop(columns=\"media_metadata\", inplace=True)\n",
    "    if \"editable\" in df.columns:\n",
    "        df.drop(columns=\"editable\", inplace=True)\n",
    "    return df\n",
    "    \n",
    "        \n",
    "        \n",
    "def create_database(database, json_fp, comments_per_chunk, chunk_size, columns_to_drop):\n",
    "    \"\"\"\n",
    "    args:\n",
    "        database            - sqlite databse object\n",
    "        json_fp             - filepath to .json raw comments file\n",
    "        comments_per_chunk  - number of comments to store in temporary .json files \n",
    "        chunk_size          - size of chunks for the pd.read_json() function\n",
    "        columns_to_drop     - columns to drop \n",
    "    \"\"\"\n",
    "    print(\"\\n######## File: {}\".format(json_fp))\n",
    "    n_comments_total = count_objects_in_file(filepath=json_fp)\n",
    "    n_chunks = (n_comments_total // comments_per_chunk) + 1\n",
    "    print(\"Contains {} comments - dividing into {} chunks\".format(n_comments_total, n_chunks))\n",
    "    \n",
    "    for idx in range(0, n_chunks):\n",
    "        start = int(idx * comments_per_chunk)\n",
    "        end = int(start + comments_per_chunk)\n",
    "        print(\"\\r   File chunk {}: Extracting and writing comments {} - {}\".format(idx+1, start, end), end='')\n",
    "        comments = extract_subset(filepath=json_fp, start=start, end=end)\n",
    "        df = pd.DataFrame(comments)\n",
    "        df = df.drop(columns=columns_to_drop)\n",
    "        df = drop_additional_columns(df=df)\n",
    "        df = df.astype(dtype_dict)\n",
    "        # Load the chunk into temporary .json file\n",
    "        df.to_json(\"data/db_chunk.json\", orient='records', lines=True)\n",
    "        write_to_database(\n",
    "            db_conn=database, \n",
    "            json_fp=\"data/db_chunk.json\", \n",
    "            chunk_size=chunk_size\n",
    "        )\n",
    "        \n",
    "        \n",
    "def run_database_builder(input_path, db_path, drop_cols, comments_per_chunk=500000, chunk_size=100000):\n",
    "    \"\"\"\n",
    "    args:\n",
    "        input_path          - filepath to .json raw comments file\n",
    "        db_path             - filepath to database file\n",
    "        drop_cols           - columns to drop \n",
    "        comments_per_chunk  - number of comments to store in temporary .json files \n",
    "        chunk_size          - size of chunks for the pd.read_json() function\n",
    "    \"\"\"\n",
    "    \n",
    "    conn = create_engine('sqlite:///'+db_path)\n",
    "\n",
    "    create_database(\n",
    "        database=conn, \n",
    "        json_fp=input_path,\n",
    "        comments_per_chunk=comments_per_chunk, \n",
    "        chunk_size=chunk_size,\n",
    "        columns_to_drop=drop_cols\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f1da12d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_ROOT = \"/media/cameron/Seagate Basic/datasets/reddit/archives_decompressed/*/*.json\"\n",
    "DATA_DEST = \"data/RC_2020_database.db\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "614d4428",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/media/cameron/Seagate Basic/datasets/reddit/archives_decompressed/mar_2020/RC_2020-03-15.json\n"
     ]
    }
   ],
   "source": [
    "data_files = sorted(glob.glob(DATA_ROOT))\n",
    "print(data_files[43])\n",
    "# n_comments = count_objects_in_file(filepath=data_files[0])\n",
    "# print(n_comments)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5f67879",
   "metadata": {},
   "source": [
    "Specify some irrelevant columns to drop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1ffe17b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_cols = [\"all_awardings\", \n",
    "             \"associated_award\", \n",
    "             \"author_flair_css_class\", \n",
    "             \"author_flair_richtext\",\n",
    "             \"author_flair_background_color\",\n",
    "             \"author_flair_text_color\",\n",
    "             \"author_flair_type\",\n",
    "             \"author_patreon_flair\",\n",
    "             \"author_flair_template_id\",\n",
    "             \"author_premium\",\n",
    "             \"can_gild\", \n",
    "             \"collapsed\",\n",
    "             \"collapsed_because_crowd_control\",\n",
    "             \"collapsed_reason\",\n",
    "             \"gildings\",\n",
    "             \"permalink\", \n",
    "             \"subreddit_name_prefixed\",\n",
    "             \"treatment_tags\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b3b5abf",
   "metadata": {},
   "source": [
    "Sanity check: Extract some comments from a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d89bfaa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['author', 'author_created_utc', 'author_flair_text', 'author_fullname', 'awarders', 'body', 'can_mod_post', 'controversiality', 'created_utc', 'distinguished', 'edited', 'gilded', 'id', 'is_submitter', 'link_id', 'locked', 'no_follow', 'parent_id', 'quarantined', 'removal_reason', 'retrieved_on', 'score', 'send_replies', 'stickied', 'subreddit', 'subreddit_id', 'subreddit_type', 'top_awarded_type', 'total_awards_received']\n"
     ]
    }
   ],
   "source": [
    "comments = extract_subset(filepath=data_files[43], start=0, end=100)\n",
    "df = pd.DataFrame(comments)\n",
    "df = df.drop(columns=drop_cols)\n",
    "df = drop_additional_columns(df=df)\n",
    "df = df.astype(dtype_dict)\n",
    "print(df.columns.values.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "640ed5a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>author_created_utc</th>\n",
       "      <th>author_flair_text</th>\n",
       "      <th>author_fullname</th>\n",
       "      <th>awarders</th>\n",
       "      <th>body</th>\n",
       "      <th>can_mod_post</th>\n",
       "      <th>controversiality</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>distinguished</th>\n",
       "      <th>edited</th>\n",
       "      <th>gilded</th>\n",
       "      <th>id</th>\n",
       "      <th>is_submitter</th>\n",
       "      <th>link_id</th>\n",
       "      <th>locked</th>\n",
       "      <th>no_follow</th>\n",
       "      <th>parent_id</th>\n",
       "      <th>quarantined</th>\n",
       "      <th>removal_reason</th>\n",
       "      <th>retrieved_on</th>\n",
       "      <th>score</th>\n",
       "      <th>send_replies</th>\n",
       "      <th>stickied</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>subreddit_id</th>\n",
       "      <th>subreddit_type</th>\n",
       "      <th>top_awarded_type</th>\n",
       "      <th>total_awards_received</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sadosmurf</td>\n",
       "      <td>1.394162e+09</td>\n",
       "      <td>United States</td>\n",
       "      <td>t2_flarf</td>\n",
       "      <td>[]</td>\n",
       "      <td>/r/signupsforpay has multiple offers to get pa...</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>1584230400</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>fkiwnmw</td>\n",
       "      <td>False</td>\n",
       "      <td>t3_fimbpe</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>t3_fimbpe</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>1591810444</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>SwagBucks</td>\n",
       "      <td>t5_2qw4c</td>\n",
       "      <td>public</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ChemicalAssistance</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>t2_4ho1ljfk</td>\n",
       "      <td>[]</td>\n",
       "      <td>Is Gregory awake yet?</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>1584230400</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>fkiwnmx</td>\n",
       "      <td>False</td>\n",
       "      <td>t3_fins0l</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>t3_fins0l</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>1591810444</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>ufc</td>\n",
       "      <td>t5_2qsev</td>\n",
       "      <td>public</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>88Trumans</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>t2_zlan5xu</td>\n",
       "      <td>[]</td>\n",
       "      <td>Actually, I have to be REMINDED to eat.  Does ...</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>1584230400</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>fkiwnmy</td>\n",
       "      <td>False</td>\n",
       "      <td>t3_filbbo</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>t3_filbbo</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>1591810444</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>My600lbLife</td>\n",
       "      <td>t5_38ycw</td>\n",
       "      <td>public</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[deleted]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[]</td>\n",
       "      <td>oh my god that looks amazing!</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>1584230400</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>fkiwnmz</td>\n",
       "      <td>False</td>\n",
       "      <td>t3_fira32</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>t3_fira32</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>1591810444</td>\n",
       "      <td>8</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>twentyonepilots</td>\n",
       "      <td>t5_2u0fp</td>\n",
       "      <td>public</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>nice-scores</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>t2_5rj1cdoq</td>\n",
       "      <td>[]</td>\n",
       "      <td>ùì∑ùì≤ùì¨ùìÆ ‚òú(Ôæü„ÉÆÔæü‚òú)\\n#Nice Leaderboard\\n**1.** `u/Gil...</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>1584230400</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>fkiwnn0</td>\n",
       "      <td>False</td>\n",
       "      <td>t3_fimjgn</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>t1_fkiw7y6</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>1591810444</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>Metroid</td>\n",
       "      <td>t5_2rrd6</td>\n",
       "      <td>public</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               author  author_created_utc author_flair_text author_fullname  \\\n",
       "0           sadosmurf        1.394162e+09     United States        t2_flarf   \n",
       "1  ChemicalAssistance                 NaN              None     t2_4ho1ljfk   \n",
       "2           88Trumans                 NaN              None      t2_zlan5xu   \n",
       "3           [deleted]                 NaN              None            None   \n",
       "4         nice-scores                 NaN              None     t2_5rj1cdoq   \n",
       "\n",
       "  awarders                                               body  can_mod_post  \\\n",
       "0       []  /r/signupsforpay has multiple offers to get pa...         False   \n",
       "1       []                              Is Gregory awake yet?         False   \n",
       "2       []  Actually, I have to be REMINDED to eat.  Does ...         False   \n",
       "3       []                      oh my god that looks amazing!         False   \n",
       "4       []  ùì∑ùì≤ùì¨ùìÆ ‚òú(Ôæü„ÉÆÔæü‚òú)\\n#Nice Leaderboard\\n**1.** `u/Gil...         False   \n",
       "\n",
       "   controversiality  created_utc distinguished edited  gilded       id  \\\n",
       "0                 0   1584230400          None  False       0  fkiwnmw   \n",
       "1                 0   1584230400          None  False       0  fkiwnmx   \n",
       "2                 0   1584230400          None  False       0  fkiwnmy   \n",
       "3                 0   1584230400          None  False       0  fkiwnmz   \n",
       "4                 0   1584230400          None  False       0  fkiwnn0   \n",
       "\n",
       "   is_submitter    link_id  locked  no_follow   parent_id  quarantined  \\\n",
       "0         False  t3_fimbpe   False       True   t3_fimbpe        False   \n",
       "1         False  t3_fins0l   False       True   t3_fins0l        False   \n",
       "2         False  t3_filbbo   False       True   t3_filbbo        False   \n",
       "3         False  t3_fira32   False      False   t3_fira32        False   \n",
       "4         False  t3_fimjgn   False       True  t1_fkiw7y6        False   \n",
       "\n",
       "  removal_reason  retrieved_on  score  send_replies  stickied  \\\n",
       "0           None    1591810444      1          True     False   \n",
       "1           None    1591810444      2          True     False   \n",
       "2           None    1591810444      2          True     False   \n",
       "3           None    1591810444      8          True     False   \n",
       "4           None    1591810444      1          True     False   \n",
       "\n",
       "         subreddit subreddit_id subreddit_type top_awarded_type  \\\n",
       "0        SwagBucks     t5_2qw4c         public             None   \n",
       "1              ufc     t5_2qsev         public             None   \n",
       "2      My600lbLife     t5_38ycw         public             None   \n",
       "3  twentyonepilots     t5_2u0fp         public             None   \n",
       "4          Metroid     t5_2rrd6         public             None   \n",
       "\n",
       "   total_awards_received  \n",
       "0                      0  \n",
       "1                      0  \n",
       "2                      0  \n",
       "3                      0  \n",
       "4                      0  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1fee2eda",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/media/cameron/Seagate Basic/datasets/reddit/archives_decompressed/mar_2020/RC_2020-03-15.json',\n",
       " '/media/cameron/Seagate Basic/datasets/reddit/archives_decompressed/mar_2020/RC_2020-03-16.json',\n",
       " '/media/cameron/Seagate Basic/datasets/reddit/archives_decompressed/mar_2020/RC_2020-03-17.json',\n",
       " '/media/cameron/Seagate Basic/datasets/reddit/archives_decompressed/mar_2020/RC_2020-03-18.json',\n",
       " '/media/cameron/Seagate Basic/datasets/reddit/archives_decompressed/mar_2020/RC_2020-03-19.json',\n",
       " '/media/cameron/Seagate Basic/datasets/reddit/archives_decompressed/mar_2020/RC_2020-03-20.json',\n",
       " '/media/cameron/Seagate Basic/datasets/reddit/archives_decompressed/mar_2020/RC_2020-03-21.json',\n",
       " '/media/cameron/Seagate Basic/datasets/reddit/archives_decompressed/mar_2020/RC_2020-03-22.json',\n",
       " '/media/cameron/Seagate Basic/datasets/reddit/archives_decompressed/mar_2020/RC_2020-03-23.json',\n",
       " '/media/cameron/Seagate Basic/datasets/reddit/archives_decompressed/mar_2020/RC_2020-03-24.json',\n",
       " '/media/cameron/Seagate Basic/datasets/reddit/archives_decompressed/mar_2020/RC_2020-03-25.json',\n",
       " '/media/cameron/Seagate Basic/datasets/reddit/archives_decompressed/mar_2020/RC_2020-03-26.json',\n",
       " '/media/cameron/Seagate Basic/datasets/reddit/archives_decompressed/mar_2020/RC_2020-03-27.json',\n",
       " '/media/cameron/Seagate Basic/datasets/reddit/archives_decompressed/mar_2020/RC_2020-03-28.json',\n",
       " '/media/cameron/Seagate Basic/datasets/reddit/archives_decompressed/mar_2020/RC_2020-03-29.json',\n",
       " '/media/cameron/Seagate Basic/datasets/reddit/archives_decompressed/mar_2020/RC_2020-03-30.json',\n",
       " '/media/cameron/Seagate Basic/datasets/reddit/archives_decompressed/mar_2020/RC_2020-03-31.json',\n",
       " '/media/cameron/Seagate Basic/datasets/reddit/archives_decompressed/may_2020/RC_2020-05-01.json',\n",
       " '/media/cameron/Seagate Basic/datasets/reddit/archives_decompressed/may_2020/RC_2020-05-02.json',\n",
       " '/media/cameron/Seagate Basic/datasets/reddit/archives_decompressed/may_2020/RC_2020-05-03.json',\n",
       " '/media/cameron/Seagate Basic/datasets/reddit/archives_decompressed/may_2020/RC_2020-05-04.json',\n",
       " '/media/cameron/Seagate Basic/datasets/reddit/archives_decompressed/may_2020/RC_2020-05-05.json',\n",
       " '/media/cameron/Seagate Basic/datasets/reddit/archives_decompressed/may_2020/RC_2020-05-06.json',\n",
       " '/media/cameron/Seagate Basic/datasets/reddit/archives_decompressed/may_2020/RC_2020-05-07.json',\n",
       " '/media/cameron/Seagate Basic/datasets/reddit/archives_decompressed/may_2020/RC_2020-05-08.json',\n",
       " '/media/cameron/Seagate Basic/datasets/reddit/archives_decompressed/may_2020/RC_2020-05-09.json',\n",
       " '/media/cameron/Seagate Basic/datasets/reddit/archives_decompressed/may_2020/RC_2020-05-10.json',\n",
       " '/media/cameron/Seagate Basic/datasets/reddit/archives_decompressed/may_2020/RC_2020-05-11.json',\n",
       " '/media/cameron/Seagate Basic/datasets/reddit/archives_decompressed/may_2020/RC_2020-05-12.json',\n",
       " '/media/cameron/Seagate Basic/datasets/reddit/archives_decompressed/may_2020/RC_2020-05-13.json',\n",
       " '/media/cameron/Seagate Basic/datasets/reddit/archives_decompressed/may_2020/RC_2020-05-14.json',\n",
       " '/media/cameron/Seagate Basic/datasets/reddit/archives_decompressed/may_2020/RC_2020-05-15.json',\n",
       " '/media/cameron/Seagate Basic/datasets/reddit/archives_decompressed/may_2020/RC_2020-05-16.json',\n",
       " '/media/cameron/Seagate Basic/datasets/reddit/archives_decompressed/may_2020/RC_2020-05-17.json',\n",
       " '/media/cameron/Seagate Basic/datasets/reddit/archives_decompressed/may_2020/RC_2020-05-18.json',\n",
       " '/media/cameron/Seagate Basic/datasets/reddit/archives_decompressed/may_2020/RC_2020-05-19.json',\n",
       " '/media/cameron/Seagate Basic/datasets/reddit/archives_decompressed/may_2020/RC_2020-05-20.json',\n",
       " '/media/cameron/Seagate Basic/datasets/reddit/archives_decompressed/may_2020/RC_2020-05-21.json',\n",
       " '/media/cameron/Seagate Basic/datasets/reddit/archives_decompressed/may_2020/RC_2020-05-22.json',\n",
       " '/media/cameron/Seagate Basic/datasets/reddit/archives_decompressed/may_2020/RC_2020-05-23.json',\n",
       " '/media/cameron/Seagate Basic/datasets/reddit/archives_decompressed/may_2020/RC_2020-05-24.json',\n",
       " '/media/cameron/Seagate Basic/datasets/reddit/archives_decompressed/may_2020/RC_2020-05-25.json',\n",
       " '/media/cameron/Seagate Basic/datasets/reddit/archives_decompressed/may_2020/RC_2020-05-26.json',\n",
       " '/media/cameron/Seagate Basic/datasets/reddit/archives_decompressed/may_2020/RC_2020-05-27.json',\n",
       " '/media/cameron/Seagate Basic/datasets/reddit/archives_decompressed/may_2020/RC_2020-05-28.json',\n",
       " '/media/cameron/Seagate Basic/datasets/reddit/archives_decompressed/may_2020/RC_2020-05-29.json',\n",
       " '/media/cameron/Seagate Basic/datasets/reddit/archives_decompressed/may_2020/RC_2020-05-30.json',\n",
       " '/media/cameron/Seagate Basic/datasets/reddit/archives_decompressed/may_2020/RC_2020-05-31.json']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_files[43:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81d6f54d",
   "metadata": {},
   "source": [
    "last read: RC_2020-03-14.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c7adb8d1",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "######## File: /media/cameron/Seagate Basic/datasets/reddit/archives_decompressed/mar_2020/RC_2020-03-15.json\n",
      "Contains 4963077 comments - dividing into 10 chunks\n",
      "   File chunk 10: Extracting and writing comments 4500000 - 5000000\n",
      "Finished. Time: 15.8 min\n",
      "\n",
      "######## File: /media/cameron/Seagate Basic/datasets/reddit/archives_decompressed/mar_2020/RC_2020-03-16.json\n",
      "Contains 5393741 comments - dividing into 11 chunks\n",
      "   File chunk 11: Extracting and writing comments 5000000 - 5500000\n",
      "Finished. Time: 22.5 min\n",
      "\n",
      "######## File: /media/cameron/Seagate Basic/datasets/reddit/archives_decompressed/mar_2020/RC_2020-03-17.json\n",
      "Contains 5333263 comments - dividing into 11 chunks\n",
      "   File chunk 11: Extracting and writing comments 5000000 - 5500000\n",
      "Finished. Time: 19.9 min\n",
      "\n",
      "######## File: /media/cameron/Seagate Basic/datasets/reddit/archives_decompressed/mar_2020/RC_2020-03-18.json\n",
      "Contains 5518891 comments - dividing into 12 chunks\n",
      "   File chunk 12: Extracting and writing comments 5500000 - 6000000\n",
      "Finished. Time: 22.2 min\n",
      "\n",
      "######## File: /media/cameron/Seagate Basic/datasets/reddit/archives_decompressed/mar_2020/RC_2020-03-19.json\n",
      "Contains 5494065 comments - dividing into 11 chunks\n",
      "   File chunk 11: Extracting and writing comments 5000000 - 5500000\n",
      "Finished. Time: 22.9 min\n",
      "\n",
      "######## File: /media/cameron/Seagate Basic/datasets/reddit/archives_decompressed/mar_2020/RC_2020-03-20.json\n",
      "Contains 5426485 comments - dividing into 11 chunks\n",
      "   File chunk 11: Extracting and writing comments 5000000 - 5500000\n",
      "Finished. Time: 17.6 min\n",
      "\n",
      "######## File: /media/cameron/Seagate Basic/datasets/reddit/archives_decompressed/mar_2020/RC_2020-03-21.json\n",
      "Contains 5019710 comments - dividing into 11 chunks\n",
      "   File chunk 11: Extracting and writing comments 5000000 - 5500000\n",
      "Finished. Time: 15.0 min\n",
      "\n",
      "######## File: /media/cameron/Seagate Basic/datasets/reddit/archives_decompressed/mar_2020/RC_2020-03-22.json\n",
      "Contains 4932866 comments - dividing into 10 chunks\n",
      "   File chunk 10: Extracting and writing comments 4500000 - 5000000\n",
      "Finished. Time: 15.0 min\n",
      "\n",
      "######## File: /media/cameron/Seagate Basic/datasets/reddit/archives_decompressed/mar_2020/RC_2020-03-23.json\n",
      "Contains 5382970 comments - dividing into 11 chunks\n",
      "   File chunk 11: Extracting and writing comments 5000000 - 5500000\n",
      "Finished. Time: 16.3 min\n",
      "\n",
      "######## File: /media/cameron/Seagate Basic/datasets/reddit/archives_decompressed/mar_2020/RC_2020-03-24.json\n",
      "Contains 5537709 comments - dividing into 12 chunks\n",
      "   File chunk 12: Extracting and writing comments 5500000 - 6000000\n",
      "Finished. Time: 17.4 min\n",
      "\n",
      "######## File: /media/cameron/Seagate Basic/datasets/reddit/archives_decompressed/mar_2020/RC_2020-03-25.json\n",
      "Contains 5695358 comments - dividing into 12 chunks\n",
      "   File chunk 12: Extracting and writing comments 5500000 - 6000000\n",
      "Finished. Time: 18.8 min\n",
      "\n",
      "######## File: /media/cameron/Seagate Basic/datasets/reddit/archives_decompressed/mar_2020/RC_2020-03-26.json\n",
      "Contains 5661184 comments - dividing into 12 chunks\n",
      "   File chunk 12: Extracting and writing comments 5500000 - 6000000\n",
      "Finished. Time: 18.3 min\n",
      "\n",
      "######## File: /media/cameron/Seagate Basic/datasets/reddit/archives_decompressed/mar_2020/RC_2020-03-27.json\n",
      "Contains 5673054 comments - dividing into 12 chunks\n",
      "   File chunk 12: Extracting and writing comments 5500000 - 6000000\n",
      "Finished. Time: 31.1 min\n",
      "\n",
      "######## File: /media/cameron/Seagate Basic/datasets/reddit/archives_decompressed/mar_2020/RC_2020-03-28.json\n",
      "Contains 5256474 comments - dividing into 11 chunks\n",
      "   File chunk 11: Extracting and writing comments 5000000 - 5500000\n",
      "Finished. Time: 24.4 min\n",
      "\n",
      "######## File: /media/cameron/Seagate Basic/datasets/reddit/archives_decompressed/mar_2020/RC_2020-03-29.json\n",
      "Contains 5325157 comments - dividing into 11 chunks\n",
      "   File chunk 11: Extracting and writing comments 5000000 - 5500000\n",
      "Finished. Time: 26.7 min\n",
      "\n",
      "######## File: /media/cameron/Seagate Basic/datasets/reddit/archives_decompressed/mar_2020/RC_2020-03-30.json\n",
      "Contains 5650998 comments - dividing into 12 chunks\n",
      "   File chunk 12: Extracting and writing comments 5500000 - 6000000\n",
      "Finished. Time: 26.5 min\n",
      "\n",
      "######## File: /media/cameron/Seagate Basic/datasets/reddit/archives_decompressed/mar_2020/RC_2020-03-31.json\n",
      "Contains 5881478 comments - dividing into 12 chunks\n",
      "   File chunk 12: Extracting and writing comments 5500000 - 6000000\n",
      "Finished. Time: 18.9 min\n",
      "\n",
      "######## File: /media/cameron/Seagate Basic/datasets/reddit/archives_decompressed/may_2020/RC_2020-05-01.json\n",
      "Contains 6159862 comments - dividing into 13 chunks\n",
      "   File chunk 13: Extracting and writing comments 6000000 - 6500000\n",
      "Finished. Time: 20.4 min\n",
      "\n",
      "######## File: /media/cameron/Seagate Basic/datasets/reddit/archives_decompressed/may_2020/RC_2020-05-02.json\n",
      "Contains 5585236 comments - dividing into 12 chunks\n",
      "   File chunk 12: Extracting and writing comments 5500000 - 6000000\n",
      "Finished. Time: 18.6 min\n",
      "\n",
      "######## File: /media/cameron/Seagate Basic/datasets/reddit/archives_decompressed/may_2020/RC_2020-05-03.json\n",
      "Contains 5582958 comments - dividing into 12 chunks\n",
      "   File chunk 12: Extracting and writing comments 5500000 - 6000000\n",
      "Finished. Time: 19.1 min\n",
      "\n",
      "######## File: /media/cameron/Seagate Basic/datasets/reddit/archives_decompressed/may_2020/RC_2020-05-04.json\n",
      "Contains 6038405 comments - dividing into 13 chunks\n",
      "   File chunk 13: Extracting and writing comments 6000000 - 6500000\n",
      "Finished. Time: 25.9 min\n",
      "\n",
      "######## File: /media/cameron/Seagate Basic/datasets/reddit/archives_decompressed/may_2020/RC_2020-05-05.json\n",
      "Contains 6167994 comments - dividing into 13 chunks\n",
      "   File chunk 13: Extracting and writing comments 6000000 - 6500000\n",
      "Finished. Time: 38.6 min\n",
      "\n",
      "######## File: /media/cameron/Seagate Basic/datasets/reddit/archives_decompressed/may_2020/RC_2020-05-06.json\n",
      "Contains 6294659 comments - dividing into 13 chunks\n",
      "   File chunk 13: Extracting and writing comments 6000000 - 6500000\n",
      "Finished. Time: 42.1 min\n",
      "\n",
      "######## File: /media/cameron/Seagate Basic/datasets/reddit/archives_decompressed/may_2020/RC_2020-05-07.json\n",
      "Contains 6346951 comments - dividing into 13 chunks\n",
      "   File chunk 13: Extracting and writing comments 6000000 - 6500000\n",
      "Finished. Time: 36.1 min\n",
      "\n",
      "######## File: /media/cameron/Seagate Basic/datasets/reddit/archives_decompressed/may_2020/RC_2020-05-08.json\n",
      "Contains 6132122 comments - dividing into 13 chunks\n",
      "   File chunk 13: Extracting and writing comments 6000000 - 6500000\n",
      "Finished. Time: 24.8 min\n",
      "\n",
      "######## File: /media/cameron/Seagate Basic/datasets/reddit/archives_decompressed/may_2020/RC_2020-05-09.json\n",
      "Contains 5702204 comments - dividing into 12 chunks\n",
      "   File chunk 12: Extracting and writing comments 5500000 - 6000000\n",
      "Finished. Time: 22.1 min\n",
      "\n",
      "######## File: /media/cameron/Seagate Basic/datasets/reddit/archives_decompressed/may_2020/RC_2020-05-10.json\n",
      "Contains 5766468 comments - dividing into 12 chunks\n",
      "   File chunk 12: Extracting and writing comments 5500000 - 6000000\n",
      "Finished. Time: 21.4 min\n",
      "\n",
      "######## File: /media/cameron/Seagate Basic/datasets/reddit/archives_decompressed/may_2020/RC_2020-05-11.json\n",
      "Contains 6182558 comments - dividing into 13 chunks\n",
      "   File chunk 13: Extracting and writing comments 6000000 - 6500000\n",
      "Finished. Time: 26.2 min\n",
      "\n",
      "######## File: /media/cameron/Seagate Basic/datasets/reddit/archives_decompressed/may_2020/RC_2020-05-12.json\n",
      "Contains 6496881 comments - dividing into 13 chunks\n",
      "   File chunk 13: Extracting and writing comments 6000000 - 6500000\n",
      "Finished. Time: 25.9 min\n",
      "\n",
      "######## File: /media/cameron/Seagate Basic/datasets/reddit/archives_decompressed/may_2020/RC_2020-05-13.json\n",
      "Contains 6323314 comments - dividing into 13 chunks\n",
      "   File chunk 13: Extracting and writing comments 6000000 - 6500000\n",
      "Finished. Time: 24.8 min\n",
      "\n",
      "######## File: /media/cameron/Seagate Basic/datasets/reddit/archives_decompressed/may_2020/RC_2020-05-14.json\n",
      "Contains 6564687 comments - dividing into 14 chunks\n",
      "   File chunk 14: Extracting and writing comments 6500000 - 7000000\n",
      "Finished. Time: 24.1 min\n",
      "\n",
      "######## File: /media/cameron/Seagate Basic/datasets/reddit/archives_decompressed/may_2020/RC_2020-05-15.json\n",
      "Contains 6296551 comments - dividing into 13 chunks\n",
      "   File chunk 13: Extracting and writing comments 6000000 - 6500000\n",
      "Finished. Time: 22.5 min\n",
      "\n",
      "######## File: /media/cameron/Seagate Basic/datasets/reddit/archives_decompressed/may_2020/RC_2020-05-16.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contains 5864991 comments - dividing into 12 chunks\n",
      "   File chunk 12: Extracting and writing comments 5500000 - 6000000\n",
      "Finished. Time: 23.5 min\n",
      "\n",
      "######## File: /media/cameron/Seagate Basic/datasets/reddit/archives_decompressed/may_2020/RC_2020-05-17.json\n",
      "Contains 5775570 comments - dividing into 12 chunks\n",
      "   File chunk 12: Extracting and writing comments 5500000 - 6000000\n",
      "Finished. Time: 21.0 min\n",
      "\n",
      "######## File: /media/cameron/Seagate Basic/datasets/reddit/archives_decompressed/may_2020/RC_2020-05-18.json\n",
      "Contains 6293784 comments - dividing into 13 chunks\n",
      "   File chunk 13: Extracting and writing comments 6000000 - 6500000\n",
      "Finished. Time: 23.2 min\n",
      "\n",
      "######## File: /media/cameron/Seagate Basic/datasets/reddit/archives_decompressed/may_2020/RC_2020-05-19.json\n",
      "Contains 6372347 comments - dividing into 13 chunks\n",
      "   File chunk 13: Extracting and writing comments 6000000 - 6500000\n",
      "Finished. Time: 22.5 min\n",
      "\n",
      "######## File: /media/cameron/Seagate Basic/datasets/reddit/archives_decompressed/may_2020/RC_2020-05-20.json\n",
      "Contains 6039779 comments - dividing into 13 chunks\n",
      "   File chunk 13: Extracting and writing comments 6000000 - 6500000\n",
      "Finished. Time: 21.3 min\n",
      "\n",
      "######## File: /media/cameron/Seagate Basic/datasets/reddit/archives_decompressed/may_2020/RC_2020-05-21.json\n",
      "Contains 6346630 comments - dividing into 13 chunks\n",
      "   File chunk 13: Extracting and writing comments 6000000 - 6500000\n",
      "Finished. Time: 21.5 min\n",
      "\n",
      "######## File: /media/cameron/Seagate Basic/datasets/reddit/archives_decompressed/may_2020/RC_2020-05-22.json\n",
      "Contains 6251786 comments - dividing into 13 chunks\n",
      "   File chunk 13: Extracting and writing comments 6000000 - 6500000\n",
      "Finished. Time: 25.0 min\n",
      "\n",
      "######## File: /media/cameron/Seagate Basic/datasets/reddit/archives_decompressed/may_2020/RC_2020-05-23.json\n",
      "Contains 5716906 comments - dividing into 12 chunks\n",
      "   File chunk 12: Extracting and writing comments 5500000 - 6000000\n",
      "Finished. Time: 24.0 min\n",
      "\n",
      "######## File: /media/cameron/Seagate Basic/datasets/reddit/archives_decompressed/may_2020/RC_2020-05-24.json\n",
      "Contains 5752649 comments - dividing into 12 chunks\n",
      "   File chunk 12: Extracting and writing comments 5500000 - 6000000\n",
      "Finished. Time: 26.4 min\n",
      "\n",
      "######## File: /media/cameron/Seagate Basic/datasets/reddit/archives_decompressed/may_2020/RC_2020-05-25.json\n",
      "Contains 5987654 comments - dividing into 12 chunks\n",
      "   File chunk 12: Extracting and writing comments 5500000 - 6000000\n",
      "Finished. Time: 28.1 min\n",
      "\n",
      "######## File: /media/cameron/Seagate Basic/datasets/reddit/archives_decompressed/may_2020/RC_2020-05-26.json\n",
      "Contains 6348030 comments - dividing into 13 chunks\n",
      "   File chunk 13: Extracting and writing comments 6000000 - 6500000\n",
      "Finished. Time: 29.1 min\n",
      "\n",
      "######## File: /media/cameron/Seagate Basic/datasets/reddit/archives_decompressed/may_2020/RC_2020-05-27.json\n",
      "Contains 6440360 comments - dividing into 13 chunks\n",
      "   File chunk 13: Extracting and writing comments 6000000 - 6500000\n",
      "Finished. Time: 27.6 min\n",
      "\n",
      "######## File: /media/cameron/Seagate Basic/datasets/reddit/archives_decompressed/may_2020/RC_2020-05-28.json\n",
      "Contains 6606814 comments - dividing into 14 chunks\n",
      "   File chunk 14: Extracting and writing comments 6500000 - 7000000\n",
      "Finished. Time: 30.8 min\n",
      "\n",
      "######## File: /media/cameron/Seagate Basic/datasets/reddit/archives_decompressed/may_2020/RC_2020-05-29.json\n",
      "Contains 6517104 comments - dividing into 14 chunks\n",
      "   File chunk 14: Extracting and writing comments 6500000 - 7000000\n",
      "Finished. Time: 29.9 min\n",
      "\n",
      "######## File: /media/cameron/Seagate Basic/datasets/reddit/archives_decompressed/may_2020/RC_2020-05-30.json\n",
      "Contains 5914075 comments - dividing into 12 chunks\n",
      "   File chunk 12: Extracting and writing comments 5500000 - 6000000\n",
      "Finished. Time: 26.0 min\n",
      "\n",
      "######## File: /media/cameron/Seagate Basic/datasets/reddit/archives_decompressed/may_2020/RC_2020-05-31.json\n",
      "Contains 6124450 comments - dividing into 13 chunks\n",
      "   File chunk 13: Extracting and writing comments 6000000 - 6500000\n",
      "Finished. Time: 26.9 min\n"
     ]
    }
   ],
   "source": [
    "for idx, data_file in enumerate(data_files[43:]):\n",
    "    t_start = time()\n",
    "    run_database_builder(\n",
    "        input_path=data_file, \n",
    "        db_path=DATA_DEST, \n",
    "        drop_cols=drop_cols,\n",
    "    )\n",
    "    t_end = time()\n",
    "    t_iter = (t_end-t_start)/60\n",
    "    print(\"\\nFinished. Time: {:.1f} min\".format(t_iter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a9f26f8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# t_start = time()\n",
    "# run_database_builder(\n",
    "#     input_path=data_files[101], \n",
    "#     db_path=DATA_DEST, \n",
    "#     drop_cols=drop_cols,\n",
    "# )\n",
    "# t_end = time()\n",
    "# t_iter = (t_end-t_start)/60\n",
    "# print(\"Finished. Time: {:.1f} min\".format(t_iter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b970d169",
   "metadata": {},
   "outputs": [],
   "source": [
    "DB_NAME = DATA_DEST\n",
    "conn = sqlite3.connect(DB_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "efc3be36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>author_created_utc</th>\n",
       "      <th>author_flair_text</th>\n",
       "      <th>author_fullname</th>\n",
       "      <th>awarders</th>\n",
       "      <th>body</th>\n",
       "      <th>can_mod_post</th>\n",
       "      <th>controversiality</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>distinguished</th>\n",
       "      <th>edited</th>\n",
       "      <th>gilded</th>\n",
       "      <th>id</th>\n",
       "      <th>is_submitter</th>\n",
       "      <th>link_id</th>\n",
       "      <th>locked</th>\n",
       "      <th>no_follow</th>\n",
       "      <th>parent_id</th>\n",
       "      <th>quarantined</th>\n",
       "      <th>removal_reason</th>\n",
       "      <th>retrieved_on</th>\n",
       "      <th>score</th>\n",
       "      <th>send_replies</th>\n",
       "      <th>stickied</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>subreddit_id</th>\n",
       "      <th>subreddit_type</th>\n",
       "      <th>top_awarded_type</th>\n",
       "      <th>total_awards_received</th>\n",
       "      <th>author_cakeday</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[deleted]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[]</td>\n",
       "      <td>[removed]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1585699200</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>fm2kad9</td>\n",
       "      <td>0</td>\n",
       "      <td>t3_fsjjok</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>t1_fm2jwbi</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>1592786643</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>FragileWhiteRedditor</td>\n",
       "      <td>t5_mcrlm</td>\n",
       "      <td>public</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>aegisbur</td>\n",
       "      <td>1.441387e+09</td>\n",
       "      <td>None</td>\n",
       "      <td>t2_q5akk</td>\n",
       "      <td>[]</td>\n",
       "      <td>like Yu-Gi-Oh or magic</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1585699200</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>fm2kada</td>\n",
       "      <td>0</td>\n",
       "      <td>t3_fsnxj5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>t3_fsnxj5</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>1592786643</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>lgbt</td>\n",
       "      <td>t5_2qhh7</td>\n",
       "      <td>public</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[deleted]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[]</td>\n",
       "      <td>[removed]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1585699200</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>fm2kadb</td>\n",
       "      <td>0</td>\n",
       "      <td>t3_fso8p5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>t3_fso8p5</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>1592786643</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>wallstreetbets</td>\n",
       "      <td>t5_2th52</td>\n",
       "      <td>public</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>DanielAdams6969</td>\n",
       "      <td>1.411679e+09</td>\n",
       "      <td>None</td>\n",
       "      <td>t2_ikmp4</td>\n",
       "      <td>[]</td>\n",
       "      <td>wait what they kicked her out?!</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1585699200</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>fm2kadc</td>\n",
       "      <td>0</td>\n",
       "      <td>t3_fsbcrt</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>t1_fm2dmta</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>1592786643</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>youtubehaiku</td>\n",
       "      <td>t5_2tqlz</td>\n",
       "      <td>public</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Hokirob</td>\n",
       "      <td>1.541870e+09</td>\n",
       "      <td>None</td>\n",
       "      <td>t2_2kp55zr0</td>\n",
       "      <td>[]</td>\n",
       "      <td>I‚Äôm a 1.865S EB guy myself.  I know some guys ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1585699200</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>fm2kadd</td>\n",
       "      <td>0</td>\n",
       "      <td>t3_fsnxga</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>t3_fsnxga</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>1592786643</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>EggsIncCoOp</td>\n",
       "      <td>t5_hc3ix</td>\n",
       "      <td>public</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            author  author_created_utc author_flair_text author_fullname  \\\n",
       "0        [deleted]                 NaN              None            None   \n",
       "1         aegisbur        1.441387e+09              None        t2_q5akk   \n",
       "2        [deleted]                 NaN              None            None   \n",
       "3  DanielAdams6969        1.411679e+09              None        t2_ikmp4   \n",
       "4          Hokirob        1.541870e+09              None     t2_2kp55zr0   \n",
       "\n",
       "  awarders                                               body  can_mod_post  \\\n",
       "0       []                                          [removed]             0   \n",
       "1       []                             like Yu-Gi-Oh or magic             0   \n",
       "2       []                                          [removed]             0   \n",
       "3       []                    wait what they kicked her out?!             0   \n",
       "4       []  I‚Äôm a 1.865S EB guy myself.  I know some guys ...             0   \n",
       "\n",
       "   controversiality  created_utc distinguished  edited  gilded       id  \\\n",
       "0                 0   1585699200          None       0       0  fm2kad9   \n",
       "1                 0   1585699200          None       0       0  fm2kada   \n",
       "2                 0   1585699200          None       0       0  fm2kadb   \n",
       "3                 0   1585699200          None       0       0  fm2kadc   \n",
       "4                 0   1585699200          None       0       0  fm2kadd   \n",
       "\n",
       "   is_submitter    link_id  locked  no_follow   parent_id  quarantined  \\\n",
       "0             0  t3_fsjjok       0          1  t1_fm2jwbi            0   \n",
       "1             0  t3_fsnxj5       0          1   t3_fsnxj5            0   \n",
       "2             0  t3_fso8p5       0          1   t3_fso8p5            0   \n",
       "3             0  t3_fsbcrt       0          0  t1_fm2dmta            0   \n",
       "4             0  t3_fsnxga       0          1   t3_fsnxga            0   \n",
       "\n",
       "  removal_reason  retrieved_on  score  send_replies  stickied  \\\n",
       "0           None    1592786643      2             1         0   \n",
       "1           None    1592786643      1             1         0   \n",
       "2           None    1592786643      1             1         0   \n",
       "3           None    1592786643      4             1         0   \n",
       "4           None    1592786643      1             1         0   \n",
       "\n",
       "              subreddit subreddit_id subreddit_type top_awarded_type  \\\n",
       "0  FragileWhiteRedditor     t5_mcrlm         public             None   \n",
       "1                  lgbt     t5_2qhh7         public             None   \n",
       "2        wallstreetbets     t5_2th52         public             None   \n",
       "3          youtubehaiku     t5_2tqlz         public             None   \n",
       "4           EggsIncCoOp     t5_hc3ix         public             None   \n",
       "\n",
       "   total_awards_received author_cakeday  \n",
       "0                      0           None  \n",
       "1                      0           None  \n",
       "2                      0           None  \n",
       "3                      0           None  \n",
       "4                      0           None  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_sql_query(\"\"\"SELECT * FROM reddit_comments LIMIT 100\"\"\", conn)\n",
    "df = df.drop(columns=[\"index\"])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a915285",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
