{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2375a4f9",
   "metadata": {},
   "source": [
    "# Subreddit Mapping\n",
    "\n",
    "Large portions of the code here is from Leland McInnes' fantastic tutorial [here](https://github.com/lmcinnes/subreddit_mapping/blob/master/Subreddit%20Mapping%20and%20Analysis.ipynb). I also provide some running commentary to breifly describe important steps. \n",
    "\n",
    "Following the tutorial, we use truncated SVD (with a sparse COO matrix) for linear dimensionality reduction and LargeVis for non-linear reduction. It is a great demonstration of two powerful dimensionality reduction algorithms on a dataset too large for standard PCA and t-SNE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "496dc693",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import adjustText\n",
    "import subprocess\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import scipy.sparse as ss\n",
    "from os.path import isfile\n",
    "from sqlalchemy import create_engine\n",
    "from sklearn.utils import check_array\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.manifold import TSNE\n",
    "from hdbscan import HDBSCAN\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "\n",
    "import bokeh\n",
    "from bokeh.plotting import figure, show, output_notebook, output_file\n",
    "from bokeh.models import HoverTool, ColumnDataSource, value, CustomJS, DataRange1d\n",
    "from bokeh.models.mappers import LinearColorMapper\n",
    "from bokeh.transform import linear_cmap\n",
    "from bokeh.palettes import plasma\n",
    "from bokeh.events import MouseWheel\n",
    "from collections import OrderedDict\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.patches import Rectangle\n",
    "from matplotlib.lines import Line2D\n",
    "from matplotlib.gridspec import GridSpec\n",
    "from IPython.display import clear_output\n",
    "from ipywidgets import interact_manual, fixed, Text\n",
    "\n",
    "# sns.set_context('poster')\n",
    "# sns.set_style('white')\n",
    "plt.rc('xtick',labelsize=16)\n",
    "plt.rc('ytick',labelsize=16)\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "423ccabe",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = pd.read_csv(\"data/RC_2020_3_months_subreddit_overlaps.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de701d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed2fffdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa4d44bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2585d203",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of pairwise commenter overlaps: {}\".format(len(raw_data)))\n",
    "print(\"t1_subreddit unique subreddits: {}\".format(len(raw_data[\"t1_subreddit\"].unique())))\n",
    "print(\"t2_subreddit unique subreddits: {}\".format(len(raw_data[\"t2_subreddit\"].unique())))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca9908a1",
   "metadata": {},
   "source": [
    "Rank the subreddits so that they are indexed in order of popularity. Popularity is defined by the total number of unique commenters in each subreddit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bd5c314",
   "metadata": {},
   "outputs": [],
   "source": [
    "subreddit_popularity = raw_data.groupby('t2_subreddit')['NumOverlaps'].sum()\n",
    "subreddits = np.array(subreddit_popularity.sort_values(ascending=False).index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5886fd4c",
   "metadata": {},
   "source": [
    "Check to see what the most popular subreddits are"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27fb9360",
   "metadata": {},
   "outputs": [],
   "source": [
    "subr_pop_df = pd.DataFrame(data={'subreddit': subreddit_popularity.sort_values(ascending=False).index,\n",
    "                       'unique_commenters': subreddit_popularity.sort_values(ascending=False).values})\n",
    "subr_pop_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f468cf08",
   "metadata": {},
   "source": [
    "Let's see how many unique commenters we have on the less popular subreddits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "971d0dfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,6))\n",
    "\n",
    "plt.title(\"popularity rank vs. unique commenters\", fontsize=18)\n",
    "plt.plot(subr_pop_df.index, subr_pop_df.unique_commenters)\n",
    "plt.xlabel(\"popularity rank\", fontsize=18)\n",
    "plt.ylabel(\"unique commenters\", fontsize=18)\n",
    "plt.yscale(\"log\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94b72a70",
   "metadata": {},
   "source": [
    "Pivot the data into a matrix such that rows and columns are both indexed by subreddits, and the entry at position (i,j) is the number of overlaps bwteen the ith and jth subreddits\n",
    "\n",
    "Create subreddit-to-integer-index map to convert the subreddit names in the table into numeric row and column indexes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d9547d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_map = dict(np.vstack([subreddits, np.arange(subreddits.shape[0])]).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7498e320",
   "metadata": {},
   "outputs": [],
   "source": [
    "values = raw_data.NumOverlaps\n",
    "row_indices = raw_data.t2_subreddit.map(index_map)\n",
    "col_indices = raw_data.t1_subreddit.map(index_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cd62a27",
   "metadata": {},
   "source": [
    "Create a sparse matrix. This format requires us to specify triples of row, column, and value for each non-zero entry in the matrix. The COO matrix constructor accepts this as a triple of arrays: the first array is the values, the second and third are arrays of row and column indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d964831e",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_matrix = ss.coo_matrix((values, (row_indices,col_indices)),\n",
    "                              shape=(subreddits.shape[0], subreddits.shape[0]),\n",
    "                              dtype=np.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54d5617d",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f6abd54",
   "metadata": {},
   "outputs": [],
   "source": [
    "conditional_prob_matrix = count_matrix.tocsr()\n",
    "conditional_prob_matrix = normalize(conditional_prob_matrix, norm='l1', copy=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b66cfbfa",
   "metadata": {},
   "source": [
    "## Converting subreddit vectors into a map\n",
    "\n",
    "### Linear dimensionality reduction down to 500 dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cc1f44f",
   "metadata": {},
   "outputs": [],
   "source": [
    "reduced_vectors = TruncatedSVD(n_components=500, random_state=1).fit_transform(conditional_prob_matrix)\n",
    "reduced_vectors = normalize(reduced_vectors, norm='l2', copy=False)\n",
    "print(reduced_vectors.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "682c21e6",
   "metadata": {},
   "source": [
    "### Nonlinear dimensionality reduction down to 2 dimensions\n",
    "\n",
    "Here we use LargeVis. It is similar to t-SNE, but scales much better with large datasets. Installation instructions can be found [here](https://github.com/rugantio/LargeVis-python3)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c5c6e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LargeVis(BaseEstimator):\n",
    "    \n",
    "    def __init__(self, n_components=2, perplexity=30.0, gamma=5,\n",
    "                 layout_samples=None, n_neighbors=None, negative_samples=5,\n",
    "                 alpha=1.0, n_cores=6, knn_prop=3, trees=50):\n",
    "        self.n_components = n_components\n",
    "        self.perplexity = perplexity\n",
    "        self.layout_samples = layout_samples\n",
    "        self.alpha = alpha\n",
    "        self.n_cores = n_cores\n",
    "        self.knn_prop = knn_prop\n",
    "        self.negative_samples = negative_samples\n",
    "        self.n_neighbors = n_neighbors\n",
    "        self.gamma = gamma\n",
    "        self.trees = trees\n",
    "        if self.n_neighbors is None:\n",
    "            self.n_neighbors = int(self.perplexity * 3)\n",
    "\n",
    "\n",
    "    def fit_transform(self, X, y=None):\n",
    "        \n",
    "        if self.layout_samples is None:\n",
    "            layout_samples = X.shape[0] / 100.0\n",
    "        else:\n",
    "            layout_samples = self.layout_samples\n",
    "            \n",
    "        X = check_array(X, dtype=np.float64)\n",
    "        np.savetxt('/tmp/largevis_input', \n",
    "                   X, header='{} {}'.format(*X.shape), \n",
    "                   comments='')\n",
    "        subprocess.check_call(['/home/cameron/LargeVis-python3/Linux/LargeVis',\n",
    "                               '-input', '/tmp/largevis_input',\n",
    "                               '-output', '/tmp/largevis_output',\n",
    "                               '-outdim', str(self.n_components),\n",
    "                               '-perp', str(self.perplexity),\n",
    "                               '-samples', str(layout_samples),\n",
    "                               '-gamma', str(self.gamma),\n",
    "                               '-prop', str(self.knn_prop),\n",
    "                               '-trees', str(self.trees),\n",
    "                               '-neigh', str(self.n_neighbors),\n",
    "                               '-alpha', str(self.alpha),\n",
    "                               '-neg', str(self.negative_samples),\n",
    "                               '-threads', str(self.n_cores)])\n",
    "        self.embedding_ = np.loadtxt('/tmp/largevis_output', skiprows=1)\n",
    "        return self.embedding_\n",
    "    \n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        self.fit_transform(X)\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13141648",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_file = 'tsne_data/RC_2020_3_months_largevis_subreddit_map.npy'\n",
    "if isfile(embed_file):\n",
    "    subreddit_map = np.load(embed_file)\n",
    "else:\n",
    "    largevis = LargeVis(perplexity=30, n_cores=16)\n",
    "    subreddit_map = largevis.fit_transform(reduced_vectors[:10000])\n",
    "    np.save(embed_file, subreddit_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd383dec",
   "metadata": {},
   "source": [
    "**Experiment**: Use t-SNE instead of LargeVis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b964ce1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# embed_file = 'tsne_data/RC_2020-04_tsne_subreddit_map.npy'\n",
    "# if isfile(embed_file):\n",
    "#     subreddit_map = np.load(embed_file)\n",
    "# else:\n",
    "#     tsne = TSNE(verbose=1, n_components=2, perplexity=50)\n",
    "#     subreddit_map = tsne.fit_transform(reduced_vectors[:10000])\n",
    "#     np.save(embed_file, subreddit_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c025584",
   "metadata": {},
   "outputs": [],
   "source": [
    "subreddit_map_df = pd.DataFrame(subreddit_map, columns=('x', 'y'))\n",
    "subreddit_map_df['subreddit'] = subreddits[:10000]\n",
    "subreddit_map_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13fa2b13",
   "metadata": {},
   "source": [
    "### Clustering the mapÂ¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31d760c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "clusterer = HDBSCAN(min_samples=5, min_cluster_size=20).fit(subreddit_map)\n",
    "cluster_ids = clusterer.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1045430",
   "metadata": {},
   "outputs": [],
   "source": [
    "subreddit_map_df['cluster'] = cluster_ids\n",
    "subreddit_map_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3e95512",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_cluster_points = len(subreddit_map_df[subreddit_map_df.cluster != -1])\n",
    "n_clusters = subreddit_map_df[\"cluster\"].max()\n",
    "score = silhouette_score(subreddit_map, cluster_ids)\n",
    "print(\"points assigned to a cluster: {}\".format(n_cluster_points))\n",
    "print(\"number of clusters: {}\".format(n_clusters))\n",
    "print(\"silhouette score: {:.4f}\".format(score))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3353598",
   "metadata": {},
   "source": [
    "### Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18b70ec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5455e491",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct a color palette and map clusters to colors\n",
    "palette = ['#777777'] + list(plasma(cluster_ids.max()))\n",
    "colormap = LinearColorMapper(palette=palette, low=-1, high=cluster_ids.max())\n",
    "color_dict = {'field': 'cluster', 'transform': colormap}\n",
    "\n",
    "# Set fill alpha globally\n",
    "subreddit_map_df['fill_alpha'] = np.exp((subreddit_map.min() - subreddit_map.max()) / 5.0) + 0.05\n",
    "\n",
    "# Build a column data source\n",
    "plot_data = ColumnDataSource(data=subreddit_map_df)\n",
    "\n",
    "# Create the figure and add tools\n",
    "fig = figure(\n",
    "    title='A Map of Subreddits',\n",
    "    plot_width = 1000,\n",
    "    plot_height = 1000,\n",
    "    tools= ('pan, wheel_zoom, box_zoom,''box_select, reset'),\n",
    "    active_scroll=u'wheel_zoom',\n",
    ")\n",
    "\n",
    "fig.add_tools(HoverTool(tooltips = OrderedDict([('subreddit', '@subreddit'), \n",
    "                                                ('cluster', '@cluster')])))\n",
    "\n",
    "# draw the subreddits as circles on the plot\n",
    "fig.circle(\n",
    "    u'x', u'y', \n",
    "    source = plot_data,\n",
    "    fill_color = color_dict, \n",
    "    line_color = None, \n",
    "    fill_alpha = 'fill_alpha',\n",
    "    size = 10, \n",
    "    hover_line_color = u'black'\n",
    ")\n",
    "\n",
    "# Custom callback for alpha adjustment\n",
    "jscode=\"\"\"\n",
    "    var data = source.data;\n",
    "    var start = cb_obj.start;\n",
    "    var end = cb_obj.end;\n",
    "    var alpha = data['fill_alpha']\n",
    "    for (var i = 0; i < alpha.length; i++) {\n",
    "         alpha[i] = Math.exp((start - end) / 5.0) + 0.05;\n",
    "    }\n",
    "    source.change.emit();\n",
    "\"\"\"\n",
    "\n",
    "callback = CustomJS(args=dict(source=plot_data), code=jscode)\n",
    "\n",
    "fig.x_range.js_on_change(\"start\", callback)\n",
    "fig.x_range.js_on_change(\"end\", callback)\n",
    "\n",
    "# configure visual elements of the plot\n",
    "# fig.title.text_font_size = value('18pt')\n",
    "fig.title.text_font_size = '18pt'\n",
    "fig.title.align = 'center'\n",
    "fig.xaxis.visible = False\n",
    "fig.yaxis.visible = False\n",
    "fig.grid.grid_line_color = None\n",
    "fig.outline_line_color = '#222222'\n",
    "\n",
    "# display the figure\n",
    "# output_file('vizualizations/april_2020_subreddit_interactive_map.html')\n",
    "show(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3b0e1c2",
   "metadata": {},
   "source": [
    "### Exploring clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25e8006f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_bounds(dataframe, subreddit):\n",
    "    # Find the cluster the subreddit belongs to\n",
    "    cluster = dataframe.cluster[dataframe.subreddit == subreddit].values[0]\n",
    "    if cluster == -1:\n",
    "        print('This subreddit was lost as noise and not in any cluster')\n",
    "        \n",
    "    # Extract the dubset of the dataframe that is the cluster\n",
    "    sub_dataframe = dataframe[dataframe.cluster == cluster]\n",
    "    \n",
    "    x_min = sub_dataframe.x.min()\n",
    "    x_max = sub_dataframe.x.max()\n",
    "    x_padding = (x_max - x_min) * 0.5\n",
    "    x_min -= x_padding\n",
    "    x_max += x_padding\n",
    "    \n",
    "    y_min = sub_dataframe.y.min()\n",
    "    y_max = sub_dataframe.y.max()\n",
    "    y_padding = (y_max - y_min) * 0.5\n",
    "    y_min -= y_padding\n",
    "    y_max += y_padding\n",
    "\n",
    "    return x_min, x_max, y_min, y_max\n",
    "\n",
    "\n",
    "def data_in_bounds(dataframe, bounds):\n",
    "    return dataframe[\n",
    "        (dataframe.x > bounds[0]) &\n",
    "        (dataframe.x < bounds[1]) &\n",
    "        (dataframe.y > bounds[2]) &\n",
    "        (dataframe.y < bounds[3])\n",
    "    ]\n",
    "\n",
    "\n",
    "def plot_cluster(dataframe, subreddit, n_labels=50, fontsize=9, dpi=100):\n",
    "    # Build a color map to match the Bokeh plot\n",
    "    colormap = dict(zip(\n",
    "        np.unique(dataframe.cluster),\n",
    "        ['#777777'] + sns.color_palette('plasma', dataframe.cluster.max() + 1).as_hex()\n",
    "    ))\n",
    "    subregion_defined = True\n",
    "    \n",
    "    # Figure and gridspec to layout axes\n",
    "    fig = plt.figure(figsize=(16,10), dpi=dpi)\n",
    "    gs = GridSpec(3, 3)\n",
    "    \n",
    "    # First axes, spanning most of the figure\n",
    "    # Contains just the points in a region \n",
    "    # around the points in the cluster\n",
    "    ax1 = plt.subplot(gs[:,:2])\n",
    "    try:\n",
    "        bounds = cluster_bounds(dataframe, subreddit)\n",
    "    except IndexError:\n",
    "        ax1.text(0.5, 0.5, 'Subreddit {} not found!'.format(subreddit), \n",
    "                 horizontalalignment='center', verticalalignment='center',\n",
    "                 transform=ax1.transAxes, fontsize=18)\n",
    "        subregion_defined = False\n",
    "    \n",
    "    if subregion_defined:\n",
    "        to_plot = data_in_bounds(dataframe, bounds)\n",
    "        ax1.scatter(to_plot.x, to_plot.y, c=to_plot.cluster.map(colormap), s=30, alpha=0.5)\n",
    "    \n",
    "        # We want to add text labels. We subsample up to 50 labels\n",
    "        # And then use adjustText to get them non-overlapping\n",
    "        text_elements = []\n",
    "        for row in to_plot.sample(n=min(len(to_plot), n_labels), random_state=0).values:\n",
    "            if row[2] != subreddit:\n",
    "                text_elements.append(ax1.text(row[0], row[1], row[2], alpha=0.5, fontsize=fontsize))\n",
    "        row = to_plot[to_plot.subreddit == subreddit].values[0]\n",
    "        text_elements.append(ax1.text(row[0], row[1], row[2], \n",
    "                                      color='g',\n",
    "                                      alpha=0.5, fontsize=11))\n",
    "        adjustText.adjust_text(text_elements, ax=ax1, lim=100,\n",
    "                               force_text=0.1, force_points=0.1,\n",
    "                               arrowprops=dict(arrowstyle=\"-\", color='k', lw=0.5))\n",
    "    \n",
    "    ax1.xaxis.set_ticklabels([])\n",
    "    ax1.yaxis.set_ticklabels([])\n",
    "\n",
    "    # Second axes, center right of the figure\n",
    "    # Plots all the data and a rectangle\n",
    "    # Showing the area selected out\n",
    "    ax2 = plt.subplot(gs[1,2])\n",
    "    ax2.scatter(dataframe.x, dataframe.y, s=20,\n",
    "                c=dataframe.cluster.map(colormap), alpha=0.05)\n",
    "    \n",
    "    if subregion_defined:\n",
    "        ax2.add_patch(Rectangle(xy=(bounds[0], bounds[2]),\n",
    "                                    width=(bounds[1] - bounds[0]),\n",
    "                                    height=(bounds[3] - bounds[2]),\n",
    "                                    edgecolor='k', facecolor='none', lw=1))\n",
    "    ax2.xaxis.set_ticklabels([])\n",
    "    ax2.yaxis.set_ticklabels([])\n",
    "    plt.tight_layout()\n",
    "\n",
    "    if subregion_defined:\n",
    "        # Now we make use of the power of matplotlib transforms\n",
    "        # to draw line from the subselected rectangle in axes2\n",
    "        # all the way to the bounds of axes1\n",
    "        trans_figure = fig.transFigure.inverted()\n",
    "\n",
    "        ax1_coord = trans_figure.transform(ax1.transAxes.transform((1,0)))\n",
    "        ax2_coord = trans_figure.transform(ax2.transData.transform((bounds[1],bounds[2])))\n",
    "        connector1 = Line2D((ax1_coord[0],ax2_coord[0]),(ax1_coord[1],ax2_coord[1]),\n",
    "                              transform=fig.transFigure, lw=1, color='k')\n",
    "        ax1_coord = trans_figure.transform(ax1.transAxes.transform((1,1)))\n",
    "        ax2_coord = trans_figure.transform(ax2.transData.transform((bounds[1],bounds[3])))\n",
    "        connector2 = Line2D((ax1_coord[0],ax2_coord[0]),(ax1_coord[1],ax2_coord[1]),\n",
    "                              transform=fig.transFigure, lw=1, color='k')\n",
    "\n",
    "        fig.lines = [connector1, connector2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da0867a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_cluster(subreddit_map_df, 'Coronavirus', n_labels=75, fontsize=11, dpi=100)\n",
    "plot_cluster(subreddit_map_df, 'Coronavirus', n_labels=25, fontsize=14, dpi=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dda2a6c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_cluster(subreddit_map_df, 'JoeRogan', n_labels=50, fontsize=11, dpi=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95656149",
   "metadata": {},
   "source": [
    "Focus on the most coherent clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5a74842",
   "metadata": {},
   "outputs": [],
   "source": [
    "coherent_clusters = np.argsort(clusterer.cluster_persistence_)[-10:][::-1]\n",
    "coherence = np.sort(clusterer.cluster_persistence_)[-10:][::-1]\n",
    "\n",
    "plt.figure(figsize=(15,5))\n",
    "\n",
    "plt.bar(np.arange(10), coherence)\n",
    "plt.gca().set_xticks(np.arange(10))\n",
    "plt.gca().set_xticklabels(coherent_clusters)\n",
    "plt.xlabel(\"Cluster\", fontsize=18)\n",
    "plt.ylabel(\"Coherence\", fontsize=18)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c36f0037",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_cluster_by_id(dataframe, cluster_id):\n",
    "    subreddits_in_cluster = np.array(dataframe.subreddit[cluster_ids == cluster_id])\n",
    "    plot_cluster(dataframe, subreddits_in_cluster[0])\n",
    "    plt.gcf().text(0.5, 0.98, 'Cluster {}'.format(cluster_id), ha='center')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8640bad9",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for idx in coherent_clusters:\n",
    "    plot_cluster_by_id(subreddit_map_df, idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04bd81ea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
